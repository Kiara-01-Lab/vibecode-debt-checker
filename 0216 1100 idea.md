This is a great problem space for Kiara to attack. Let me think through this with YC-style validation.

---

**Core insight**: The pain isn't that AI writes bad code. It's that there's no **automated way to detect and enforce consistency** across an AI-generated codebase. Human code review doesn't scale when 41% of all code is AI-generated. Linters catch syntax, not architectural drift.

---

## Idea: `vibe-check` — an OSS codebase consistency analyzer for AI-generated code

**3-second pitch**: "Catch the mess before it ships. Static analysis built for AI-generated codebases."

**What it does**: Scans a codebase and detects the specific failure modes that AI coding tools produce — not traditional lint errors, but **pattern inconsistency, architectural drift, and vibe coding debt**.

---

### What it would detect (mapped to your painpoints 1–7):

**Painpoint 1 — "Works but I don't know why"**
→ Generate a **codebase architecture map** with a "comprehension score." Flag files with no tests, no comments, and high cyclomatic complexity. Output a plain-English summary of what each module does (using LLM in the analysis pass). Essentially: "here's what your code actually does, explained to you."

**Painpoint 2 — Inconsistent patterns**
→ Detect **pattern fragmentation**: multiple data-fetching approaches (fetch vs axios vs SWR vs server actions), mixed state management (useState + Zustand + Context in same project), inconsistent error handling patterns, duplicate components doing the same thing. Report: "You have 3 different patterns for fetching data. Here's where each is used. Pick one."

**Painpoint 3 — Code grows beyond comprehension**
→ Track **codebase complexity velocity** over time (integrate with git). "Your complexity score increased 340% in the last 50 commits. These 12 files are the biggest contributors." Flag the inflection point where the codebase is entering the danger zone.

**Painpoint 4 — Vibe coding debt compounding**
→ A **debt score** that's more meaningful than SonarQube for AI codebases. Weighted metric combining: pattern inconsistency count, untested code ratio, dead code percentage, duplicate logic instances, security smell count. Track over time. CI/CD integrable.

**Painpoint 5 — Refactoring is harder than rewriting**
→ Generate **refactoring plans**: "To consolidate your data-fetching patterns, here are the 14 files that need to change and the suggested unified approach." This is the bridge between "the AI wrote spaghetti" and "here's how to untangle it."

**Painpoint 6 — Breaks on iteration**
→ **Pre-prompt context generation**: Before you prompt your AI tool, `vibe-check` generates a context file summarizing your codebase's established patterns, conventions, and architecture decisions. Feed this to Cursor/Claude Code as a rules file. Prevents the AI from contradicting itself.

**Painpoint 7 — Context window loss**
→ Same solution as above — automatically generate and maintain a `.vibe-check/context.md` file that stays current as the codebase evolves. This is the "memory" the AI tools lack.

---

### Technical shape:

**SDK/CLI**: `npx vibe-check` or `pip install vibe-check`

**Language**: Start with TypeScript (Next.js/React codebases are 80%+ of vibe-coded projects), Python later

**Architecture**:
- AST parsing layer (no LLM needed for pattern detection)
- Pattern catalog (known AI anti-patterns: mixed fetch strategies, duplicate components, etc.)
- Git integration for velocity/trend tracking
- Optional LLM pass for plain-English explanations and refactoring suggestions (use Anthropic API)
- Output: JSON report + terminal summary + optional CI check

**Integrations**:
- GitHub Action (run on every PR)
- Cursor rules generator (auto-generate `.cursorrules` from detected patterns)
- Claude Code `CLAUDE.md` generator (same for Claude Code)
- VS Code extension (later)

---

### Why this could work (YC lens):

**Market timing**: 92% of US devs using AI tools daily, 41% of code is AI-generated, but zero tooling exists specifically for AI-generated code quality. SonarQube/ESLint catch syntax, not architectural drift. This is a greenfield category.

**Wedge**: Free OSS CLI → paid cloud dashboard with team features, historical tracking, and CI/CD integration. Classic open-core model.

**Distribution**: npm/pip install. Post to r/vibecoding, r/cursor, r/nextjs. Every vibe coder who's hit "the wall" is the target user. The pain is acute and well-documented.

**Defensibility**: The pattern catalog is the moat. Every AI-generated anti-pattern you catalog makes the tool more valuable. Network effects if you aggregate anonymized pattern data across users to improve detection.

**Revenue path**: Free for individual / small repos. Paid team tier ($20-50/seat/mo) for dashboard, historical trends, CI/CD, Slack alerts. Enterprise tier for org-wide standards enforcement.

---

### What I'd validate first:

1. **Does the pattern detection actually work?** Build a prototype that scans 10 open-source vibe-coded repos (tons on GitHub) and see if the inconsistency detection produces useful, actionable output.

2. **Do vibe coders actually want this?** The worry: the people who need it most (non-technical builders) might not care about code quality until production breaks. The early adopters are more likely technical founders and senior devs supervising AI output — which is a smaller but higher-willingness-to-pay market.

3. **Is the Cursor rules / CLAUDE.md generation the real killer feature?** This might be more valuable than the analysis itself — automatically maintaining the context file that prevents AI from going off-rails.

---

### Potential pivot / adjacent ideas:

- **`vibe-shield`** — Focused purely on security scanning for AI-generated code (painpoints 8-13). Narrower scope, clearer urgency, easier to sell. "Your Supabase RLS is broken, here's how" is a more immediate pain than "your code patterns are inconsistent."

- **`vibe-context`** — Just the context/rules file generator. Super narrow, super useful. Auto-generates and maintains `.cursorrules` / `CLAUDE.md` / `.windsurfrules` by analyzing your actual codebase. Lowest effort to build, highest immediate utility.

---

Want me to spec out any of these in more detail, or prototype the pattern detection logic?
